# Виртуализация и контейнеризация

<details>
<summary>1. Введение в виртуализацию</summary>

## Задача 2

| Задача | Рекомендуемая платформа | Критерии выбора |
|--------|-------------------------|------------------|
| Высоконагруженная база данных MySQL, критичная к отказу | Физические серверы | Максимальная производительность I/O, отсутствие задержек виртуализации, прямой доступ к железу, возможности RAID |
| Различные web-приложения | Виртуализация уровня ОС (контейнеры) | Быстрое масштабирование, изоляция приложений, эффективное использование ресурсов, микросервисная архитектура |
| Windows-системы для бухгалтерского отдела | Паравиртуализация | Совместимость с 1С и т.п. платформами, централизованное управление, снапшоты для бэкапов, изоляция пользователей |
| Системы высокопроизводительных расчётов на GPU | Физические серверы | Прямой доступ к GPU, отсутствие overhead, максимальная производительность CUDA/OpenCL, контроль над драйверами и библиотеками |

## Задание 3

| Сценарий | Рекомендуемое решение | Обоснование | Альтернатива |
|----------|----------------------|-------------|--------------|
| 100 виртуальных машин на базе Linux и Windows, общие задачи, нет особых требований. Преимущественно Windows based-инфраструктура, требуется реализация программных балансировщиков нагрузки, репликации данных и автоматизированного механизма создания резервных копий. | Microsoft Hyper-V | Интеграция с AD, PowerShell, поддержка репликации, резервного копирования, масштабируемость | Proxmox VE |
| Требуется наиболее производительное бесплатное open source-решение для виртуализации небольшой (20-30 серверов) инфраструктуры на базе Linux и Windows виртуальных машин. | Proxmox VE (KVM + LXC) | Поддержка Windows и Linux, веб-интерфейс, кластеры, live migration, резервное копирование | oVirt |
| Необходимо бесплатное, максимально совместимое и производительное решение для виртуализации Windows-инфраструктуры. | Microsoft Hyper-V | Максимальная совместимость с Windows, высокая производительность, встроен в Windows Server | KVM с VirtIO-драйверами |
| Необходимо рабочее окружение для тестирования программного продукта на нескольких дистрибутивах Linux. | Docker/Docker Swarm | Легковесные контейнеры для CLI, VirtualBox для GUI, кроссплатформенность, простота настройки. Для окружений до 10 машин Kubernetes будет ресурсоемким решением. | Vagrant + VirtualBox/Libvirt. Возможен k8s.|

## Задача 4

Опишите возможные проблемы и недостатки гетерогенной среды виртуализации (использования нескольких систем управления виртуализацией одновременно) и что необходимо сделать для минимизации этих рисков и проблем. Если бы у вас был выбор, создавали бы вы гетерогенную среду или нет?

1) Проблемы гетерогенной среды виртуализации.

а) Гетерогенная среда виртуализации как NP-полная задача.

Практическая реализация гетерогенной среды виртуализации представляет собой NP-полную задачу, аналогичную по сложности «задаче рюкзака». Она требует конечного объёма ресурсов, в рамках которых необходимо «упаковать» различные системы виртуализации на хостах разных типов. Цель — достичь максимального «объёма» (т.е. количества критически необходимых разнородных систем виртуализации) при максимальной ценности элементов (например, хостов, которые по техническим или регуляторным причинам не могут быть переведены на другой тип виртуализации). Пример: миграция с Hyper-V на Proxmox при условии, что одна из систем поддерживает только Hyper-V, а другая — нет, при этом необходимо сохранить QoS, например, для отделения частной хирургии.

б) Гетерогенная среда виртуализации как NP-трудная задача.

 Сложность задачи возрастает до NP-трудной, если учитывать постоянное добавление новых элементов в инфраструктуру (роботы, дроны, IoTT, Kubernetes + OpenStack, контейнеры Astra Linux и др.), что экспоненциально увеличивает количество возможных конфигураций и решений. Дополнительную NP-сложность вносит необходимость тестирования различных гетерогенных конфигураций.
 Для таких задач целесообразным представляется применение парного тестирования.

2) Общие подходы к минимизации рисков.

• Географически распределённая (относительно пользователей) сеть мелких или средних хостов на промежуточном (middleware) уровне cloud-IoT (как пример), тем самым реализуя идею гетерогенного пула при относительной гомогенности вычислительных ресурсов (CPU, RAM, GPU) с учётом нахождения реального баланса между QoS и SLA.

• Использование atomic hosts либо официальных immutable Docker-образов, появившихся недавно.

• Использование, например, BGP, поддержание мониторинга аномалий AS_PATH и метрик QoS/SLA.

• Понимание выбора стратегии (балансировка QoS/SLA, Latency Greedy, Cost Greedy, GeKube).

• Использование адаптирующихся эволюционных алгоритмов. Для решения задач предсказания поведения гетерогенной виртуализации необходимо применять различные математические модели. Так, для линейных показателей (например, соотношение CPU/Memory, IOPS к утилизации диска и т.п.) целесообразно использовать фундаментальные методы тестирования, такие как Granger causality. В то же время, для нелинейных метрик — CPU, GPU, Memory — характерно поведение, требующее предварительного усреднения или расчёта средневзвешенных значений; в таких случаях эффективно применяются модели ARIMA или SARIMA.

• Политика, где миграция контейнера на ближайший узел будет приоритетнее его вертикального масштабирования, а одна нода не получит ресурсов больше самой ресурсоемкой.

3) Примеры репозиториев с гетерогенной средой оркестрации Kubernetes в GitHub.

- eddytruyen/heterogenous-scaling-in-k8s - включение/отключение гетерогенного масштабирования, инструменты симуляции нагрузок, создание YAML-матриц масштабирования в зависимости от присвоенного класса (Золото, Бронза), контроллер масштабирования со своим неймспейсом и подом resource-planner. Graphite для метрик (push-based). Heapster, Grafana - визуализация.

- lippertmarkus/vagrant-k8s-win-hostprocess - построенный на базе Vagrant кластер под управлением нодами типа winworker с помощью Windows HostProcess Pods, с одной стороны, и с другой - Calico/CNI для классической оркестрации линукс нод, при этом не используется установка новых Windows сервисов.

- hufs-ese-lab/MC-Kube (автономное вождение, индустриальный IoT), мониторинг на уровне eBPF c уменьшением отклика в 246 раз по сравнению со стандартными API запросами k8s, инструменты для патчинга стандартного Completely Fair (CFS) шедулера (установка SCHED_DEADLINE для подов через патчи), автоматизация рантайма (задача, которая не выполнена в дедлайн попадает в runtime_hi).

4) Проблемы выбора гетерогенной среды.

В ближайшие 20 лет, с учётом развития квантовых вычислений, нейроморфных чипов, интернета вещей (IoT) и роботизации, гетерогенная инфраструктура станет единственно возможной.
</details>
<details>
<summary>2. Применение принципов IaaC в работе с виртуальными машинами</summary>

## Задача 2

![Скриншот выполнения Задания 2](2.png)

## Задача 3

![Скриншот выполнения Задания 3](3-1.png)

![Скриншот выполнения Задания 3](3.png)

</details>

<details>
<summary>4. Оркестрация группой Docker контейнеров на примере Docker Compose</summary>


## Задача 1

[Ссылка на Репозиторий: slateecho/custom-nginx](https://hub.docker.com/repository/docker/slateecho/custom-nginx/general)

## Задача 2

![Скриншот выполнения Задания 2](5-2-2.png)

## Задача 3

Поскольку при выполнении docker attach опция --sig-proxy[=true] по умолчанию включена, происходит переадресация всех сигналов из терминала к PID 1 внутри контейнера, как если бы он был запущен в терминале. Процесс может быть настроен определённым образом отвечать на SIGINT (^C), например, игнорируя его.

![Скриншот выполнения Задания 3](5-3-1.png)

![Скриншот выполнения Задания 3](5-3-2.png)

![Скриншот выполнения Задания 3](5-3-3.png)

![Скриншот выполнения Задания 3](5-3-4.png)

Поскольку маппинг портов, как отображено на скриншоте выше, при изменении внутри контейнера параметров запуска не меняется, а в логах появляется запись "10-listen-on-ipv6-by-default.sh: info: /etc/nginx/conf.d/default.conf differs from the packaged version", не представляется возможным изменить параметры Nginx контейнера путём редактирования его conf файла внутри контейнера.

*На основе упомянутого https://www.baeldung.com/ops/assign-port-docker-container мануала изменён внутренний порт с 80 на 81

![Скриншот выполнения Задания 3](5-3-5.png)

## Задача 3

1) Был запущен файл compose.yaml (с сервисом portainer), поскольку, как указано в документации по адресу https://docs.docker.com/compose/intro/compose-application-model/, это предпочтительное название по умолчанию для запуска Docker Compose.

2) Скриншот второго контейнера nginx

![Скриншот выполнения Задания 3](5-4-2.png)

3) После удаления compose.yaml остался "сиротский" контейнер Portainer, который было предложено удалить в предупреждении.

![Скриншот выполнения Задания 3](5-4-1.png)

</details>

